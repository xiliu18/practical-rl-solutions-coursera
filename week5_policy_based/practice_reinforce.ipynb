{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in TensorFlow\n",
    "\n",
    "This notebook implements a basic reinforce algorithm a.k.a. policy gradient for CartPole env.\n",
    "\n",
    "It has been deliberately written to be as simple and human-readable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook assumes that you have [openai gym](https://github.com/openai/gym) installed.\n",
    "\n",
    "In case you're running on a server, [use xvfb](https://github.com/openai/gym#rendering-on-a-server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting virtual X frame buffer: Xvfb.\n",
      "env: DISPLAY=:1\n"
     ]
    }
   ],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if os.environ.get(\"DISPLAY\") is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7eff675ddac8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyoCXpE4Z8JLUqf8HVM6GNQJI7QkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the policy network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__. \n",
    "\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#create input variables. We only need <s,a,R> for REINFORCE\n",
    "states = tf.placeholder('float32',(None,)+state_dim,name=\"states\")\n",
    "actions = tf.placeholder('int32',name=\"action_ids\")\n",
    "cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_1 = tf.layers.dense(states, 64, tf.nn.relu)\n",
    "layer_2 = tf.layers.dense(layer_1, 64, tf.nn.relu)\n",
    "logits = tf.layers.dense(layer_2, n_actions)\n",
    "\n",
    "policy = tf.nn.softmax(logits)\n",
    "log_policy = tf.nn.log_softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utility function to pick action in one given state\n",
    "get_action_proba = lambda s: policy.eval({states:[s]})[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get probabilities for parti\n",
    "indices = tf.stack([tf.range(tf.shape(log_policy)[0]),actions],axis=-1)   \n",
    "log_policy_for_actions = tf.gather_nd(log_policy,indices) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# policy objective as in the last formula. please use mean, not sum.\n",
    "# note: you need to use log_policy_for_actions to get log probabilities for actions taken\n",
    "\n",
    "J = tf.reduce_mean(log_policy_for_actions * cumulative_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#regularize with entropy\n",
    "entropy = -tf.reduce_sum(policy*log_policy, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all network weights\n",
    "all_weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "#weight updates. maximizing J is same as minimizing -J. Adding negative entropy.\n",
    "loss = -J -0.1 * entropy\n",
    "\n",
    "update = tf.train.AdamOptimizer().minimize(loss,var_list=all_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, #rewards at each step\n",
    "                           gamma = 0.99 #discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n",
    "    R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "    \n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute R_t = r_t + gamma*R_{t+1} recurrently\n",
    "    \n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    return scipy.signal.lfilter([1], [1, float(-gamma)], rewards[::-1], axis=0)[::-1]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "assert len(get_cumulative_rewards(range(100))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,0,0,1,0],gamma=0.9),[1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,-2,3,-4,0],gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,2,3,4,0],gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_step(_states,_actions,_rewards):\n",
    "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
    "    _cumulative_rewards = get_cumulative_rewards(_rewards)\n",
    "    update.run({states:_states,actions:_actions,cumulative_rewards:_cumulative_rewards})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
    "    \n",
    "    #arrays to record session\n",
    "    states,actions,rewards = [],[],[]\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #action probabilities array aka pi(a|s)\n",
    "        action_probas = get_action_proba(s)\n",
    "        \n",
    "        a = np.random.choice(n_actions, p=action_probas)\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    train_step(states,actions,rewards)\n",
    "            \n",
    "    return sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:37.480\n",
      "mean reward:128.520\n",
      "mean reward:473.180\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "s = tf.InteractiveSession()\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    print (\"mean reward:%.3f\"%(np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "from submit import submit_cartpole\n",
    "submit_cartpole(generate_session, <EMAIL>, <TOKEN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# That's all, thank you for your attention!\n",
    "# Not having enough? There's an actor-critic waiting for you in the honor section.\n",
    "# But make sure you've seen the videos first."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
